{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMwCxVrVuKcM"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "H5T_UJwZuKcN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import io\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0I3IjWNuKcO"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2X5_G-gBuKcO"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Reviews.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yXIsNE9WuKcO",
    "outputId": "32a614d8-b8e4-4483-92ac-03d5a3ff7cbe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ProductId          UserId                      ProfileName  \\\n",
       "Id                                                                \n",
       "1   B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "2   B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "3   B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "4   B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "5   B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "    HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "Id                                                                    \n",
       "1                      1                       1      5  1303862400   \n",
       "2                      0                       0      1  1346976000   \n",
       "3                      1                       1      4  1219017600   \n",
       "4                      3                       3      2  1307923200   \n",
       "5                      0                       0      5  1350777600   \n",
       "\n",
       "                  Summary                                               Text  \n",
       "Id                                                                            \n",
       "1   Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "2       Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "3   \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "4          Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "5             Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be classifying a review as positive sentiment if it has a score greater than 3. The reviews with score less than 3 are taken as negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TbONTaXvuKcP"
   },
   "outputs": [],
   "source": [
    "df[\"review\"] = df[\"Score\"].apply(lambda x: 0 if x<4 else 1)\n",
    "\n",
    "# Retain only the columns required\n",
    "df = df[[\"Text\",\"review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568454</th>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text  review\n",
       "Id                                                               \n",
       "1       I have bought several of the Vitality canned d...       1\n",
       "2       Product arrived labeled as Jumbo Salted Peanut...       0\n",
       "3       This is a confection that has been around a fe...       1\n",
       "4       If you are looking for the secret ingredient i...       0\n",
       "5       Great taffy at a great price.  There was a wid...       1\n",
       "...                                                   ...     ...\n",
       "568450  Great for sesame chicken..this is a good if no...       1\n",
       "568451  I'm disappointed with the flavor. The chocolat...       0\n",
       "568452  These stars are small, so you can give 10-15 o...       1\n",
       "568453  These are the BEST treats for training and rew...       1\n",
       "568454  I am very satisfied ,product is as advertised,...       1\n",
       "\n",
       "[568454 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HLCHcGUuuKcQ",
    "outputId": "dabfd6dc-d1be-4e9c-90d3-9f88969c5d91"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>These stars are small, so you can give - of th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568454</th>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text  review\n",
       "Id                                                               \n",
       "1       I have bought several of the Vitality canned d...       1\n",
       "2       Product arrived labeled as Jumbo Salted Peanut...       0\n",
       "3       This is a confection that has been around a fe...       1\n",
       "4       If you are looking for the secret ingredient i...       0\n",
       "5       Great taffy at a great price.  There was a wid...       1\n",
       "...                                                   ...     ...\n",
       "568450  Great for sesame chicken..this is a good if no...       1\n",
       "568451  I'm disappointed with the flavor. The chocolat...       0\n",
       "568452  These stars are small, so you can give - of th...       1\n",
       "568453  These are the BEST treats for training and rew...       1\n",
       "568454  I am very satisfied ,product is as advertised,...       1\n",
       "\n",
       "[568454 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_processing(text):\n",
    "    text = re.sub(r'\\d+', '',text) # remove any numbers\n",
    "    text = re.sub('<.*?>','',text) # remove any html tags\n",
    "    text = re.sub(r'https?:\\\\/\\\\/.*[\\\\r\\\\n]*', '', text, flags=re.MULTILINE) # remove any URLs\n",
    "    return text\n",
    "\n",
    "df[\"Text\"] = df[\"Text\"].apply(lambda x: text_processing(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-2CFMEtuKcQ"
   },
   "source": [
    "### Balancing Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb0KVJY8uKcQ"
   },
   "source": [
    "The dataset has almost 4 times the positive reviews compared to negative reviews. To counter this we can sample only a part of the positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lIPDv0siuKcR",
    "outputId": "b9b87d46-b665-485b-f0ca-2ff2671eccdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    443777\n",
       "0    124677\n",
       "Name: review, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 for positive sentiment and 0 for negative sentiment\n",
    "df[\"review\"].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7Kbq5ShTuKcR"
   },
   "outputs": [],
   "source": [
    "positive_reviews = df[df.review == 1]\n",
    "negative_reviews = df[df.review == 0]\n",
    "\n",
    "# sample positive examples whose number is equal to the negative examples\n",
    "positive_reviews = positive_reviews.sample(n=len(negative_reviews))\n",
    "\n",
    "df = positive_reviews.append(negative_reviews).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "drUDRn8DuKcR",
    "outputId": "2a605dd8-ff87-44f6-9b4a-30f6226acebd",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I received this freezer tray along with the Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have tried many OOlong teas and this is by f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Its a really delicious earthy, herbal tea. Eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just got the product yesterday. I used it la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This was a gift for mom who had run out.  She ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249349</th>\n",
       "      <td>I just bought this soup today at my local groc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249350</th>\n",
       "      <td>This soup is mostly broth. Although it has a k...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249351</th>\n",
       "      <td>It is mostly broth, with the advertised / cup ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249352</th>\n",
       "      <td>I had ordered some of these a few months back ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249353</th>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249354 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text  review\n",
       "0       I received this freezer tray along with the Be...       1\n",
       "1       I have tried many OOlong teas and this is by f...       1\n",
       "2       Its a really delicious earthy, herbal tea. Eve...       1\n",
       "3       I just got the product yesterday. I used it la...       1\n",
       "4       This was a gift for mom who had run out.  She ...       1\n",
       "...                                                   ...     ...\n",
       "249349  I just bought this soup today at my local groc...       0\n",
       "249350  This soup is mostly broth. Although it has a k...       0\n",
       "249351  It is mostly broth, with the advertised / cup ...       0\n",
       "249352  I had ordered some of these a few months back ...       0\n",
       "249353  I'm disappointed with the flavor. The chocolat...       0\n",
       "\n",
       "[249354 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "# The data is not shuffled right now, but it can be shuffled once we call the train test split function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 249354 reviews out of which half are positive while the orher half is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "26u5dRf-uKcR"
   },
   "outputs": [],
   "source": [
    "sentences = df[\"Text\"].values\n",
    "labels = df[\"review\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now split into training and testing sets of 80% and 20% respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PCjOO4F2uKcS"
   },
   "outputs": [],
   "source": [
    "# Define training and testing sets\n",
    "train_sentences,test_sentences,train_labels,test_labels = train_test_split(sentences,labels,test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR0L8mt_uKcS"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keras `Tokenizer` class helps in tokenizing the words to form a vocabulary of all the words in the training set.\n",
    "These tokens are fed into the `Embedding` layer to output the respective embedding vectors\n",
    "\n",
    "These tokens are then put into sequences based on the sentence. `max_length` is specified to either pad the sentence or truncate it to a specified length.\n",
    "Any padding or truncation is done from the end of a sentence.\n",
    "The keyword `'post'` needs to be specified to the pad_sequences method.\n",
    "Specifying `vocab_size = 10000` tells the tokenizer to only take the most common 10000 words used in the training set.\n",
    "`max_length` and `vocab_size` are hyperparameters which can be tuned to include more data to be input to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "axzSFSDquKcS"
   },
   "outputs": [],
   "source": [
    "# dimension of the embedding layer\n",
    "embed_dim = 64\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "\n",
    "# Any word out of the vocabulary is represented as <OOV>\n",
    "oov_tok = \"<OOV>\"\n",
    "vocab_size = 10000\n",
    "max_length = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XrS2pgRFuKcS"
   },
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)\n",
    "\n",
    "# Assign tokens based on words on training set\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "# Create sequences based on tokens for the training set\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "\n",
    "# pad/truncate zeros at the end for a length of 'max_length'\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type) \n",
    "\n",
    "# similar preprocessing for test set\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the tokenizer does we first give an example sentence to it and check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is =>  The food was really bad. It lacked any sort of flavour and was a waste of money.\n",
      "\n",
      "The tokenized text is =>  [[2, 51, 17, 65, 139, 7, 4076, 99, 816, 8, 1687, 4, 17, 5, 495, 8, 260]]\n",
      "\n",
      "The padded sequence is =>  [[   2   51   17   65  139    7 4076   99  816    8 1687    4   17    5\n",
      "   495    8  260    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_text = \"The food was really bad. It lacked any sort of flavour and was a waste of money.\"\n",
    "sequence = tokenizer.texts_to_sequences(np.array([example_text]))\n",
    "padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(\"The sentence is => \",example_text)\n",
    "print()\n",
    "print(\"The tokenized text is => \",sequence)\n",
    "print()\n",
    "print(\"The padded sequence is => \",padded)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZJ3pc54uKcT"
   },
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.layers.Bidirectional` layer helps in the LSTM doing forward pass on both directions of the sequence. This helps for words in the later parts of the sentence affect the activations on earlier words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "s_a0PUD7uKcT",
    "outputId": "2a3d7b3c-184e-48bf-981e-5e90fe509a0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 400, 64)           640000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 666,017\n",
      "Trainable params: 666,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embed_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(8,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0006)\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Embedding` layer is basically a look up matrix to get the embeddings (64 dimensional in this case) of the words in the vocabulary. We can see the embeddings for the first 3 words in the vocabulary below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.41739766e-02,  3.12285628e-02, -1.02722965e-01,\n",
       "        -7.94368237e-02, -3.20303962e-02, -1.46004379e-01,\n",
       "         1.21531516e-01, -1.01161391e-01, -4.25138101e-02,\n",
       "        -1.14942245e-01, -9.70788207e-03,  3.72352712e-02,\n",
       "        -2.27977559e-02, -1.25555083e-01,  2.11090297e-02,\n",
       "        -2.50796620e-02,  3.52056473e-02, -5.37137277e-02,\n",
       "        -1.42084584e-02,  1.65980030e-02,  1.35070095e-02,\n",
       "         5.58682792e-02, -1.55708417e-01, -4.44716774e-03,\n",
       "         2.53997594e-01,  3.83338183e-02,  5.71752675e-02,\n",
       "         1.07002072e-01,  4.10854332e-02, -5.69156930e-02,\n",
       "         1.64387539e-01,  9.01966542e-03,  3.01409932e-03,\n",
       "        -1.66384019e-02,  1.31393829e-02, -5.32785244e-02,\n",
       "        -6.73323944e-02, -6.54477067e-03, -3.04315407e-02,\n",
       "         2.37717051e-02,  1.16757110e-01, -6.18470497e-02,\n",
       "         4.07477096e-02,  1.47574380e-01,  4.45311423e-03,\n",
       "         1.04876831e-01,  2.44890191e-02, -1.32776961e-01,\n",
       "         2.00850554e-02,  1.32677078e-01,  1.95405502e-02,\n",
       "         1.56674311e-02,  8.84602871e-03, -8.49154685e-03,\n",
       "         8.29918087e-02,  7.57750720e-02, -8.18352401e-02,\n",
       "         1.63297534e-01,  4.52399999e-02, -2.12445632e-02,\n",
       "         2.35335425e-01, -9.02828798e-02, -9.50859040e-02,\n",
       "         5.49778417e-02],\n",
       "       [ 1.96222542e-03, -3.58189084e-02,  2.78510572e-03,\n",
       "        -1.49392923e-02,  7.28267990e-03,  2.45393105e-02,\n",
       "         7.75767304e-03,  5.06776795e-02,  2.76454389e-02,\n",
       "        -3.22580449e-02,  5.16850734e-03, -2.95063611e-02,\n",
       "         3.14527601e-02,  7.86761381e-03, -2.20329012e-03,\n",
       "        -4.30276729e-02, -7.08183125e-02,  1.83397420e-02,\n",
       "         4.11258340e-02,  5.29868118e-02, -5.13584390e-02,\n",
       "         9.95205808e-03,  1.98438764e-02,  5.55901863e-02,\n",
       "         4.75292141e-03,  7.71811232e-02, -1.54425083e-02,\n",
       "         1.12061314e-02,  3.55547257e-02, -6.57272935e-02,\n",
       "        -4.42262739e-02, -2.93249916e-03, -3.42719350e-03,\n",
       "        -1.54369045e-02,  1.03084203e-02,  2.03176737e-02,\n",
       "         2.74755321e-02,  3.79251018e-02,  6.31418303e-02,\n",
       "        -2.00757664e-03,  3.47575955e-02, -7.35649001e-03,\n",
       "         1.88565645e-02, -4.32150625e-02, -2.19808938e-03,\n",
       "        -8.41917749e-03, -1.50156729e-02, -7.22455382e-02,\n",
       "         3.20729129e-02,  6.11465015e-02,  5.69143333e-03,\n",
       "         7.00839311e-02, -1.07791659e-03, -1.40851538e-04,\n",
       "        -1.42272105e-02,  6.15961710e-03,  3.81299816e-02,\n",
       "         4.69007380e-02,  2.49023251e-02, -2.71205883e-03,\n",
       "         2.36830395e-02,  1.84271485e-03,  1.42645469e-04,\n",
       "         6.49255440e-02],\n",
       "       [ 3.70220095e-02,  2.65771747e-02,  2.29565687e-02,\n",
       "         2.07941663e-02, -4.59626801e-02, -6.76714443e-03,\n",
       "         5.01179956e-02,  1.56831760e-02, -3.81580219e-02,\n",
       "        -1.44349048e-02,  3.59052769e-03, -7.81053165e-03,\n",
       "         1.13765514e-02,  4.49933186e-02, -7.90758710e-03,\n",
       "         2.96073430e-03, -1.95780303e-02, -4.52648252e-02,\n",
       "        -6.14984985e-03, -6.88565371e-04,  2.88235932e-03,\n",
       "        -5.01063541e-02, -3.84068415e-02,  1.02194007e-02,\n",
       "         1.47567699e-02, -4.40039374e-02,  9.04971287e-02,\n",
       "        -3.55026536e-02,  5.24607077e-02,  1.88313648e-02,\n",
       "         7.44049177e-02, -3.90460677e-02, -5.24786524e-02,\n",
       "        -5.15677631e-02,  6.96441904e-03, -4.30274047e-02,\n",
       "        -5.16845565e-03,  4.31408640e-03, -1.60436183e-02,\n",
       "        -1.11866780e-02, -1.40848486e-02,  4.52673621e-03,\n",
       "         1.82928275e-02,  4.30843048e-02, -5.88947125e-02,\n",
       "         1.70290843e-02,  1.77109297e-02,  1.27427373e-02,\n",
       "        -4.89150779e-03,  1.99786536e-02,  4.93696928e-02,\n",
       "         3.08482777e-02, -2.02554129e-02,  4.49286364e-02,\n",
       "        -2.18337327e-02, -2.43583824e-02,  3.58946025e-02,\n",
       "         5.43311378e-03, -3.08229737e-02, -1.09845223e-02,\n",
       "        -2.14276910e-02, -1.58314034e-02, -4.02282029e-02,\n",
       "        -1.14429591e-03]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()[0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2hVekLzUuKcT",
    "outputId": "15a55f48-f0b8-4a7a-d285-418c6be08bc7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1559/1559 [==============================] - 64s 41ms/step - loss: 0.4298 - accuracy: 0.8268 - val_loss: 0.3147 - val_accuracy: 0.8721\n",
      "Epoch 2/10\n",
      "1559/1559 [==============================] - 62s 40ms/step - loss: 0.3356 - accuracy: 0.8789 - val_loss: 0.2782 - val_accuracy: 0.8898\n",
      "Epoch 3/10\n",
      "1559/1559 [==============================] - 62s 40ms/step - loss: 0.2912 - accuracy: 0.8968 - val_loss: 0.2714 - val_accuracy: 0.8895\n",
      "Epoch 4/10\n",
      "1559/1559 [==============================] - 62s 40ms/step - loss: 0.2604 - accuracy: 0.9093 - val_loss: 0.2528 - val_accuracy: 0.8994\n",
      "Epoch 5/10\n",
      "1559/1559 [==============================] - 62s 40ms/step - loss: 0.2336 - accuracy: 0.9192 - val_loss: 0.2557 - val_accuracy: 0.9031\n",
      "Epoch 6/10\n",
      "1559/1559 [==============================] - 62s 40ms/step - loss: 0.2183 - accuracy: 0.9245 - val_loss: 0.2496 - val_accuracy: 0.9056\n",
      "Epoch 7/10\n",
      "1559/1559 [==============================] - 62s 40ms/step - loss: 0.1996 - accuracy: 0.9321 - val_loss: 0.2821 - val_accuracy: 0.9077\n",
      "Epoch 8/10\n",
      "1559/1559 [==============================] - 62s 40ms/step - loss: 0.1868 - accuracy: 0.9367 - val_loss: 0.2771 - val_accuracy: 0.9083\n",
      "Epoch 9/10\n",
      "1559/1559 [==============================] - 62s 40ms/step - loss: 0.1744 - accuracy: 0.9410 - val_loss: 0.3052 - val_accuracy: 0.9099\n",
      "Epoch 10/10\n",
      "1559/1559 [==============================] - 64s 41ms/step - loss: 0.1622 - accuracy: 0.9452 - val_loss: 0.2972 - val_accuracy: 0.9104\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_padded, train_labels, epochs=10, validation_data=(test_padded, test_labels),batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9T3N_6puKcT"
   },
   "source": [
    "# Get embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the following blocks produces two tsv files `vecs.tsv` and `meta.tsv`. These files can then be uploaded to [projector.tensorlow.org]() to view the embeddings in a 3d space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "g_IEYshhuKcU"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYog1Se_uKcU",
    "outputId": "e0a99f09-a89b-4585-94bb-ac77fb4055ca"
   },
   "outputs": [],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "S78eLhgUuKcU"
   },
   "outputs": [],
   "source": [
    "# Write the embeddings and their corresponding meta data into tsv files\n",
    "out_v = io.open('embeddings/vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('embeddings/meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gHq2aBLuKcU"
   },
   "source": [
    "# Predict using custom string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom strings can then be tokenized by the `Tokenizer` class and padded by `pad_sequences()` and the sentiment can be predicted using `model.predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "jZdC7utZuKcU"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model,custom_text):\n",
    "    custom_sequence = tokenizer.texts_to_sequences(np.array([custom_text]))\n",
    "    custom_padded = pad_sequences(custom_sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    if float(model.predict(custom_padded))>0.5:\n",
    "        print(\"The review has a positive sentiment :)\")\n",
    "    else:\n",
    "        print(\"The review has a negative sentiment :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "nKayP0NLuKcU"
   },
   "outputs": [],
   "source": [
    "test_string = \"I really liked the flavour the cinnamon brought out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "8J6GK_xquKcV",
    "outputId": "0fac2ed1-8d6a-4de9-cb5d-7f28e60d2910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review has a positive sentiment :)\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(model,test_string)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
